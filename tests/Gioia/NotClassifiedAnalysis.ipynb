{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Classified Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is analyzing and plotting the difference in the number of labelled reads between the outputs of different classifiers.\n",
    "In particular, we WON'T FOCUS ON RECALL but only on the number of classified reads, on the reads that are not classified by all the examinated classifiers and those that are labelled by only one or some classifiers and not by others and so on. So, we want to plot the consistency between the different outputs of the classifiers and understand if our reassignment procedure is meaningfull or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided Classifiers outputs --> The goal is boosting their recall (i.e., sensibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_250000 dataset\n",
    "classifier_path_centrifuge = \"../../Bio_Project/SimDataset/classifiers_results/strex_centrifuge_250000.res\"\n",
    "classifier_path_kraken1 = \"../../Bio_Project/SimDataset/classifiers_results/strex_kraken1_250000.res\"\n",
    "classifier_path_kraken2 = \"../../Bio_Project/SimDataset/classifiers_results/strex_kraken2_250000.res\"\n",
    "classifier_path_clark_species = \"../../Bio_Project/SimDataset/classifiers_results/strex_clark_species_250000.res\"\n",
    "classifier_path_clark_genus = \"../../Bio_Project/SimDataset/classifiers_results/strex_clark_genus_250000.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided classifiers outputs\n",
    "centrifuge = load_classifier_result(classifier_path_centrifuge)\n",
    "kraken1 = load_classifier_result(classifier_path_kraken1)\n",
    "kraken2 = load_classifier_result(classifier_path_kraken2)\n",
    "clark_species = load_classifier_result(classifier_path_clark_species)\n",
    "clark_genus = load_classifier_result(classifier_path_clark_genus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINGLE CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Single Classifier Output + LiME_binning + TOTAL Reassignment (Major Vote Rule) --> ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO START I WILL CONSIDER ONLY CENTRIFUGE CLASSIFIER \n",
    "# paths to ST classifiers outputs\n",
    "classifier_path_centrifuge_ST = \"../pythonProgram/all_250000_1.fq_strex_centrifuge_25000.totalReassignment.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ST classifiers outputs\n",
    "centrifuge_ST = load_classifier_result(classifier_path_centrifuge_ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Single Classifier Output + LiME_binning + PARTIAL Reassignment (Major Vote Rule) --> SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to SP classifiers outputs\n",
    "classifier_path_centrifuge_SP = \"../pythonProgram/all_250000_1.fq_strex_centrifuge_25000.partialReassignment.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SP classifiers outputs\n",
    "centrifuge_SP = load_classifier_result(classifier_path_centrifuge_SP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Single Classifier Output + LiME_binning + TOTAL Reassignment (Major Vote Rule) + ZERO trick --> STZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to STZ classifiers outputs\n",
    "classifier_path_centrifuge_STZ = \"../pythonProgram/all_250000_1.fq_strex_centrifuge_25000.totalReassignment.zero_version.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load STZ classifiers outputs\n",
    "centrifuge_STZ = load_classifier_result(classifier_path_centrifuge_STZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Single Classifier Output + LiME_binning + PARTIAL Reassignment (Major Vote Rule) + ZERO trick --> SPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to SPZ classifiers outputs\n",
    "classifier_path_centrifuge_SPZ = \"../pythonProgram/all_250000_1.fq_strex_centrifuge_25000.partialReassignment.zero_version.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SPZ classifiers outputs\n",
    "centrifuge_SPZ = load_classifier_result(classifier_path_centrifuge_SPZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + TOTAL Reassignment (Major Vote Rule) + VERSION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MT1 classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MT1 = \"../pythonProgram/all_250000_1.fq_multioutputfile.V1.totalReassignment.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MT1 classifier output\n",
    "classifier_MT1 = load_classifier_result(classifier_path_MT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + TOTAL Reassignment (Major Vote Rule) + VERSION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MT2 classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MT2 = \"../pythonProgram/all_250000_1.fq_multioutputfile.V2.totalReassignment.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MT2 classifier outputs\n",
    "classifier_MT2 = load_classifier_result(classifier_path_MT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + PARTIAL Reassignment (Major Vote Rule) + VERSION 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MP1 classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MP1 = \"../pythonProgram/all_250000_1.fq_multioutputfile.V1.partialReassignment.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MP1 classifier outputs\n",
    "classifier_MP1 = load_classifier_result(classifier_path_MP1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + PARTIAL Reassignment (Major Vote Rule) + VERSION 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MP2 classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MP2 = \"../pythonProgram/all_250000_1.fq_multioutputfile.V2.partialReassignment.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MP2 classifier outputs\n",
    "classifier_MP2 = load_classifier_result(classifier_path_MP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + TOTAL Reassignment (Major Vote Rule) + VERSION 1 + ZERO trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MT1Z classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MT1Z = \"../pythonProgram/all_250000_1.fq_multioutputfile.V1.totalReassignment.zero_version.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MT1Z classifier outputs\n",
    "classifier_MT1Z = load_classifier_result(classifier_path_MT1Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + TOTAL Reassignment (Major Vote Rule) + VERSION 2 + ZERO trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MT2Z classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MT2Z = \"../pythonProgram/all_250000_1.fq_multioutputfile.V2.totalReassignment.zero_version.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MT2Z classifier outputs\n",
    "classifier_MT2Z = load_classifier_result(classifier_path_MT2Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + PARTIAL Reassignment (Major Vote Rule) + VERSION 1 + ZERO trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MP1Z classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MP1Z = \"../pythonProgram/all_250000_1.fq_multioutputfile.V1.partialReassignment.zero_version.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MP1Z classifier outputs\n",
    "classifier_MP1Z = load_classifier_result(classifier_path_MP1Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifications obtained by our reassignment procedure --> Multi Classifier Output + LiME_binning + PARTIAL Reassignment (Major Vote Rule) + VERSION 2 + ZERO trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to MP2Z classifier output (kraken1 + kraken2 + Centrifuge)\n",
    "classifier_path_MP2Z = \"../pythonProgram/all_250000_1.fq_multioutputfile.V2.partialReassignment.zero_version.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MP2Z classifier outputs\n",
    "classifier_MP2Z = load_classifier_result(classifier_path_MP2Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block will be about the comparisons of the classification output obtained by applying our procedure to centrifuge classification output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the number of non labelled reads for centrifuge and centrifuge variants: ST, SP, STZ and SPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non labelled reads centrifuge:  345022\n",
      "non labelled reads centrifuge_ST:  332220\n",
      "non labelled reads centrifuge_SP:  274914\n",
      "non labelled reads centrifue_STZ:  57557\n",
      "non labelled reads centrifuge_SPZ:  57557\n"
     ]
    }
   ],
   "source": [
    "non_labelled_centrifuge = non_classified_reads(centrifuge)\n",
    "print(\"non labelled reads centrifuge: \", non_labelled_centrifuge[1])\n",
    "\n",
    "non_labelled_centrifuge_ST = non_classified_reads(centrifuge_ST)\n",
    "print(\"non labelled reads centrifuge_ST: \", non_labelled_centrifuge_ST[1])\n",
    "\n",
    "non_labelled_centrifuge_SP = non_classified_reads(centrifuge_SP)\n",
    "print(\"non labelled reads centrifuge_SP: \", non_labelled_centrifuge_SP[1])\n",
    "\n",
    "non_labelled_centrifuge_STZ = non_classified_reads(centrifuge_STZ)\n",
    "print(\"non labelled reads centrifue_STZ: \", non_labelled_centrifuge_STZ[1])\n",
    "\n",
    "non_labelled_centrifuge_SPZ = non_classified_reads(centrifuge_SPZ)\n",
    "print(\"non labelled reads centrifuge_SPZ: \", non_labelled_centrifuge_SPZ[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find common non labelled reads between:\n",
    "- centrifuge and centrifuge_ST, \n",
    "- centrifuge and centrifuge_SP, \n",
    "- centrifuge and centrifuge_STZ,\n",
    "- centrifuge and centrifuge_SPZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common non labelled reads ST:  274974\n",
      "Common non labelled reads SP:  274914\n",
      "Common non labelled reads STZ:  57557\n",
      "Common non labelled reads SPZ:  57557\n"
     ]
    }
   ],
   "source": [
    "common_non_labelled_reads_ST = intersection(non_labelled_centrifuge[0], non_labelled_centrifuge_ST[0])\n",
    "print(\"Common non labelled reads ST: \", len(common_non_labelled_reads_ST))\n",
    "\n",
    "common_non_labelled_reads_SP = intersection(non_labelled_centrifuge[0], non_labelled_centrifuge_SP[0])\n",
    "print(\"Common non labelled reads SP: \", len(common_non_labelled_reads_SP))\n",
    "      \n",
    "common_non_labelled_reads_STZ = intersection(non_labelled_centrifuge[0], non_labelled_centrifuge_STZ[0])\n",
    "print(\"Common non labelled reads STZ: \", len(common_non_labelled_reads_STZ))\n",
    "      \n",
    "common_non_labelled_reads_SPZ = intersection(non_labelled_centrifuge[0], non_labelled_centrifuge_SPZ[0])\n",
    "print(\"Common non labelled reads SPZ: \", len(common_non_labelled_reads_SPZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between the non classified reads by:\n",
    "   - centrifuge_ST and centrifuge_SP,\n",
    "   - centrifuge_STZ and centrifuge_SPZ\n",
    "   - centrifuge_ST and centrifuge_STZ,\n",
    "   - centrifuge_SP and centrifuge_SPZ\n",
    "### --> How many unlabelled reads they have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common non labelled reads b/t centrifuge_ST and centrifuge_SP:  273710\n"
     ]
    }
   ],
   "source": [
    "# intersection ST and SP\n",
    "common_non_labelled_reads_ST_SP = intersection(non_labelled_centrifuge_ST[0], non_labelled_centrifuge_SP[0])\n",
    "print(\"Common non labelled reads b/t centrifuge_ST and centrifuge_SP: \", len(common_non_labelled_reads_ST_SP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common non labelled reads b/t centrifuge_STZ and centrifuge_SPZ:  57557\n"
     ]
    }
   ],
   "source": [
    "# intersection STZ and SPZ\n",
    "common_non_labelled_reads_STZ_SPZ = intersection(non_labelled_centrifuge_STZ[0], non_labelled_centrifuge_SPZ[0])\n",
    "print(\"Common non labelled reads b/t centrifuge_STZ and centrifuge_SPZ: \", len(common_non_labelled_reads_STZ_SPZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common non labelled reads b/t centrifuge_ST and centrifuge_STZ:  57557\n"
     ]
    }
   ],
   "source": [
    "# intersection ST and STZ\n",
    "common_non_labelled_reads_ST_STZ = intersection(non_labelled_centrifuge_ST[0], non_labelled_centrifuge_STZ[0])\n",
    "print(\"Common non labelled reads b/t centrifuge_ST and centrifuge_STZ: \", len(common_non_labelled_reads_ST_STZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common non labelled reads b/t centrifuge_SP and centrifuge_SPZ:  57557\n"
     ]
    }
   ],
   "source": [
    "# intersection SP and SPZ\n",
    "common_non_labelled_reads_SP_SPZ = intersection(non_labelled_centrifuge_SP[0], non_labelled_centrifuge_SPZ[0])\n",
    "print(\"Common non labelled reads b/t centrifuge_SP and centrifuge_SPZ: \", len(common_non_labelled_reads_SP_SPZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrifuge and Centrifug_ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not labelled by centrifuge labelled by centrifuge_ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads not labelled by centrifuge that are labelled by centrifuge_ST:  70048\n"
     ]
    }
   ],
   "source": [
    "difference_std_ST = difference(non_labelled_centrifuge[0], non_labelled_centrifuge_ST[0])\n",
    "print(\"Number of reads not labelled by centrifuge that are labelled by centrifuge_ST: \", len(difference_std_ST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not labelled by centrifuge_ST, labelled by centrifuge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads not labelled by centrifuge_ST that are labelled by centrifuge:  57246\n"
     ]
    }
   ],
   "source": [
    "difference_ST_std = difference(non_labelled_centrifuge_ST[0], non_labelled_centrifuge[0])\n",
    "print(\"Number of reads not labelled by centrifuge_ST that are labelled by centrifuge: \", len(difference_ST_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if numbers are OK and everything \"mi torna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345022 --> OK\n",
      "332220 --> OK\n"
     ]
    }
   ],
   "source": [
    "if len(difference_std_ST) + len(common_non_labelled_reads_ST) == non_labelled_centrifuge[1]:\n",
    "    print(non_labelled_centrifuge[1], end = ' ')\n",
    "    print(\"--> OK\")\n",
    "if len(difference_ST_std) + len(common_non_labelled_reads_ST) == non_labelled_centrifuge_ST[1]:\n",
    "    print(non_labelled_centrifuge_ST[1], end = ' ')\n",
    "    print(\"--> OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxid_1042876.661 --> centrifuge:  0  centrifuge_ST:  1196325\n",
      "taxid_1042876.971 --> centrifuge:  0  centrifuge_ST:  1235689\n",
      "taxid_1042876.1601 --> centrifuge:  0  centrifuge_ST:  1196325\n",
      "taxid_1042876.1782 --> centrifuge:  0  centrifuge_ST:  1215088\n",
      "taxid_1042876.2003 --> centrifuge:  0  centrifuge_ST:  264730\n",
      "taxid_1042876.2451 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.2557 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.2995 --> centrifuge:  0  centrifuge_ST:  1215088\n",
      "taxid_1042876.3043 --> centrifuge:  0  centrifuge_ST:  76759\n",
      "taxid_1042876.3566 --> centrifuge:  0  centrifuge_ST:  316\n",
      "taxid_1042876.3622 --> centrifuge:  0  centrifuge_ST:  287\n",
      "taxid_1042876.3768 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.4128 --> centrifuge:  0  centrifuge_ST:  76759\n",
      "taxid_1042876.4491 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.5099 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.5330 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.5759 --> centrifuge:  0  centrifuge_ST:  286\n",
      "taxid_1042876.6018 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.6632 --> centrifuge:  0  centrifuge_ST:  286\n",
      "taxid_1042876.6753 --> centrifuge:  0  centrifuge_ST:  76759\n",
      " \n",
      "taxid_1042876.373 --> centrifuge_ST 0  centrifuge:  76869\n",
      "taxid_1042876.528 --> centrifuge_ST 0  centrifuge:  479433\n",
      "taxid_1042876.977 --> centrifuge_ST 0  centrifuge:  1031711\n",
      "taxid_1042876.1081 --> centrifuge_ST 0  centrifuge:  1331671\n",
      "taxid_1042876.1283 --> centrifuge_ST 0  centrifuge:  76759\n",
      "taxid_1042876.1427 --> centrifuge_ST 0  centrifuge:  710685\n",
      "taxid_1042876.1443 --> centrifuge_ST 0  centrifuge:  1001585\n",
      "taxid_1042876.1571 --> centrifuge_ST 0  centrifuge:  504346\n",
      "taxid_1042876.2285 --> centrifuge_ST 0  centrifuge:  1214101\n",
      "taxid_1042876.2858 --> centrifuge_ST 0  centrifuge:  286\n",
      "taxid_1042876.3150 --> centrifuge_ST 0  centrifuge:  76759\n",
      "taxid_1042876.3310 --> centrifuge_ST 0  centrifuge:  76114\n",
      "taxid_1042876.3327 --> centrifuge_ST 0  centrifuge:  286\n",
      "taxid_1042876.3387 --> centrifuge_ST 0  centrifuge:  710696\n",
      "taxid_1042876.3610 --> centrifuge_ST 0  centrifuge:  365046\n",
      "taxid_1042876.3855 --> centrifuge_ST 0  centrifuge:  76869\n",
      "taxid_1042876.3955 --> centrifuge_ST 0  centrifuge:  286\n",
      "taxid_1042876.4094 --> centrifuge_ST 0  centrifuge:  1331671\n",
      "taxid_1042876.4195 --> centrifuge_ST 0  centrifuge:  35\n",
      "taxid_1042876.4427 --> centrifuge_ST 0  centrifuge:  1331671\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, 20):\n",
    "    print(difference_std_ST[i], \"--> centrifuge: \", centrifuge[difference_std_ST[i]], \" centrifuge_ST: \", centrifuge_ST[difference_std_ST[i]])\n",
    "print(\" \")\n",
    "for i in range (0, 20):\n",
    "    print(difference_ST_std[i], \"--> centrifuge_ST\", centrifuge_ST[difference_ST_std[i]], \" centrifuge: \", centrifuge[difference_ST_std[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrifuge and Centrifuge_SP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not labelled by centrifuge labelled by centrifuge_SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads not labelled by centrifuge that are labelled by centrifuge_SP:  70108\n"
     ]
    }
   ],
   "source": [
    "difference_std_SP = difference(non_labelled_centrifuge[0], non_labelled_centrifuge_SP[0])\n",
    "print(\"Number of reads not labelled by centrifuge that are labelled by centrifuge_SP: \", len(difference_std_SP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not labelled by centrifuge_SP labelled by centrifuge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads not labelled by centrifuge_SP that are labelled by centrifuge:  0\n"
     ]
    }
   ],
   "source": [
    "difference_SP_std = difference(non_labelled_centrifuge_SP[0], non_labelled_centrifuge[0])\n",
    "print(\"Number of reads not labelled by centrifuge_SP that are labelled by centrifuge: \", len(difference_SP_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if numbers are OK and everything \"mi torna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345022 --> OK\n",
      "274914 --> OK\n"
     ]
    }
   ],
   "source": [
    "if len(difference_std_SP) + len(common_non_labelled_reads_SP) == non_labelled_centrifuge[1]:\n",
    "    print(non_labelled_centrifuge[1], end = ' ')\n",
    "    print(\"--> OK\")\n",
    "if len(difference_SP_std) + len(common_non_labelled_reads_SP) == non_labelled_centrifuge_SP[1]:\n",
    "    print(non_labelled_centrifuge_SP[1], end = ' ')\n",
    "    print(\"--> OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxid_1042876.661 --> centrifuge:  0  centrifuge_ST:  1196325\n",
      "taxid_1042876.770 --> centrifuge:  0  centrifuge_ST:  710685\n",
      "taxid_1042876.971 --> centrifuge:  0  centrifuge_ST:  1235689\n",
      "taxid_1042876.1534 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.1601 --> centrifuge:  0  centrifuge_ST:  1196325\n",
      "taxid_1042876.1782 --> centrifuge:  0  centrifuge_ST:  1215088\n",
      "taxid_1042876.2003 --> centrifuge:  0  centrifuge_ST:  303\n",
      "taxid_1042876.2451 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.2557 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.2995 --> centrifuge:  0  centrifuge_ST:  1215088\n",
      "taxid_1042876.3768 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.4456 --> centrifuge:  0  centrifuge_ST:  640132\n",
      "taxid_1042876.4491 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.5099 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.5330 --> centrifuge:  0  centrifuge_ST:  487\n",
      "taxid_1042876.5888 --> centrifuge:  0  centrifuge_ST:  1211579\n",
      "taxid_1042876.6018 --> centrifuge:  0  centrifuge_ST:  1331671\n",
      "taxid_1042876.6051 --> centrifuge:  0  centrifuge_ST:  1224\n",
      "taxid_1042876.6679 --> centrifuge:  0  centrifuge_ST:  1211579\n",
      "taxid_1042876.6753 --> centrifuge:  0  centrifuge_ST:  76759\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, 20):\n",
    "    print(difference_std_SP[i], \"--> centrifuge: \", centrifuge[difference_std_SP[i]], \" centrifuge_ST: \", centrifuge_SP[difference_std_SP[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute inprovement in the number of unlabelled reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CENTRIFUGE CLASSIFICATION improvement in number of classified reads with ST:  12802\n",
      "CENTRIFUGE CLASSIFICATION improvement in number of classified reads with SP:  70108\n",
      "CENTRIFUGE CLASSIFICATION improvement in number of classified reads with STZ:  287465\n",
      "CENTRIFUGE CLASSIFICATION improvement in number of classified reads with SPZ:  287465\n"
     ]
    }
   ],
   "source": [
    "# centrifuge vs centrifuge_ST\n",
    "improvement_ST = non_labelled_centrifuge[1] - non_labelled_centrifuge_ST[1]\n",
    "print(\"CENTRIFUGE CLASSIFICATION improvement in number of classified reads with ST: \", improvement_ST)\n",
    "\n",
    "# centrifuge vs centrifuge_SP\n",
    "improvement_SP = non_labelled_centrifuge[1] - non_labelled_centrifuge_SP[1]\n",
    "print(\"CENTRIFUGE CLASSIFICATION improvement in number of classified reads with SP: \", improvement_SP)\n",
    "\n",
    "# centrifuge vs centrifuge_STZ\n",
    "improvement_STZ = non_labelled_centrifuge[1] - non_labelled_centrifuge_STZ[1]\n",
    "print(\"CENTRIFUGE CLASSIFICATION improvement in number of classified reads with STZ: \", improvement_STZ)\n",
    "\n",
    "# centrifuge vs centrifuge_SPZ\n",
    "improvement_SPZ = non_labelled_centrifuge[1] - non_labelled_centrifuge_SPZ[1]\n",
    "print(\"CENTRIFUGE CLASSIFICATION improvement in number of classified reads with SPZ: \", improvement_SPZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATOLOGICAL READS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get general dict:  1.8606147766113281\n",
      "Length of general_dict:  402268\n"
     ]
    }
   ],
   "source": [
    "dict_list = [non_labelled_centrifuge[0], non_labelled_centrifuge_ST[0], non_labelled_centrifuge_SP[0], non_labelled_centrifuge_STZ[0], non_labelled_centrifuge_SPZ[0]]\n",
    "\n",
    "start = time.time()\n",
    "gen_dict = find_general_dict(dict_list)\n",
    "end = time.time()\n",
    "print(\"Time to get general dict: \", end - start)\n",
    "print(\"Length of general_dict: \", len(gen_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patological_reads:  57557\n"
     ]
    }
   ],
   "source": [
    "# patological reads are those that are unlabelled by all classifier in dict_list\n",
    "patological_reads = []\n",
    "\n",
    "num_classifiers = len(dict_list)\n",
    "\n",
    "for read in gen_dict.keys():\n",
    "    if len(gen_dict[read]) == num_classifiers:\n",
    "        patological_reads.append(read)\n",
    "    elif len(gen_dict[read]) > num_classifiers or len(gen_dict[read]) == 0:\n",
    "        print(\"Some problem occurs\")\n",
    "\n",
    "print(\"Number of patological_reads: \", len(patological_reads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patological read:  taxid_1042876.16 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.19 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.39 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.65 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.70 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.76 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.112 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.125 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.138 --> ['0', '1', '2', '3', '4']\n",
      "patological read:  taxid_1042876.149 --> ['0', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "# for debugging\n",
    "for i in range(0, 10):\n",
    "    print(\"patological read: \", patological_reads[i], end = ' --> ')\n",
    "    print(gen_dict[patological_reads[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOAL (of the following section): finding the clusters where patological reads appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../Bio_Project/SimDataset/reads_datasets/all_250000_1.fq\"\n",
    "clusters_path =  \"../../Bio_Project/results/LiME_binning_all_250000_1/all_250000_1+RC.fasta.a16.t20.txt\"\n",
    "# clusters_path =  \"../../Bio_Project/results/LiME_binning_all_250000_paired/all_250000_1+RC.fasta.a16.t20.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path, False)\n",
    "clusters = load_clusters_result(clusters_path)\n",
    "\n",
    "# dict of key value pairs where the key is the id of the read and value is the correspondent cluster\n",
    "final_dataset = build_dataset(dataset, clusters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125000\n",
      "taxid_1042876.1   0\n",
      "taxid_1042876.2   1\n",
      "taxid_1042876.3   2\n",
      "taxid_1042876.4   3\n",
      "taxid_1042876.5   4\n",
      "taxid_1042876.6   5\n",
      "taxid_1042876.7   6\n",
      "taxid_1042876.8   7\n",
      "taxid_1042876.9   8\n",
      "taxid_1042876.10   9\n",
      "taxid_1042876.11   10\n"
     ]
    }
   ],
   "source": [
    "print(len(final_dataset))\n",
    "\n",
    "i = 0\n",
    "for read, cluster in final_dataset.items():\n",
    "    if i <= 10:\n",
    "        print(read, \" \", cluster)\n",
    "    else:\n",
    "        break\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patological clusters data structure built in  0.3005995750427246\n"
     ]
    }
   ],
   "source": [
    "patological_clusters = {}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for read in patological_reads:\n",
    "    \n",
    "    if final_dataset[read] not in patological_clusters.keys():\n",
    "        patological_clusters[final_dataset[read]] = []\n",
    "        patological_clusters[final_dataset[read]].append(read)\n",
    "        \n",
    "    else:\n",
    "        patological_clusters[final_dataset[read]].append(read)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Patological clusters data structure built in \", end - start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patological clusters:  28622\n",
      "Number of patological clusters:  28622\n",
      "Most problematic cluster:  631964\n",
      "Number of patological reads in most problematic cluster:  39\n"
     ]
    }
   ],
   "source": [
    "# for debugging\n",
    "print(\"Number of patological clusters: \", len(patological_clusters))\n",
    "\n",
    "patological_clusters_cardinality = {}\n",
    "\n",
    "for cluster, reads in patological_clusters.items():\n",
    "    patological_clusters_cardinality[cluster] = len(reads)\n",
    "\n",
    "# for debugging \n",
    "print(\"Number of patological clusters: \", len(patological_clusters_cardinality))\n",
    "\n",
    "min_cardinality = min(patological_clusters_cardinality.values())\n",
    "max_cardinality = max(patological_clusters_cardinality.values())\n",
    "\n",
    "for cluster in patological_clusters_cardinality.keys():\n",
    "    if patological_clusters_cardinality[cluster] == max_cardinality:\n",
    "        max_cluster = cluster\n",
    "\n",
    "print(\"Most problematic cluster: \", max_cluster)\n",
    "print(\"Number of patological reads in most problematic cluster: \", max_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "tot_patological_reads_among_clusters = 0\n",
    "for cardinality in patological_clusters_cardinality.values():\n",
    "    tot_patological_reads_among_clusters = tot_patological_reads_among_clusters + cardinality\n",
    "    \n",
    "if (tot_patological_reads_among_clusters == len(patological_reads)):\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(tot_patological_reads_among_clusters, \"should be equal to \", len(patological_reads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tuples = sorted(patological_clusters_cardinality.items(), key=lambda item: item[1], reverse= True)\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631964   39\n",
      "33240   38\n",
      "34631   36\n",
      "32328   32\n",
      "631960   32\n",
      "341960   31\n",
      "375631   30\n",
      "444710   30\n",
      "32051   29\n",
      "32407   29\n",
      "306636   29\n",
      "32906   28\n",
      "33018   28\n",
      "444572   28\n",
      "446737   28\n",
      "32312   27\n",
      "31647   26\n",
      "138932   26\n",
      "447747   26\n",
      "33624   25\n",
      "375358   25\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for cluster, cardinality in sorted_dict.items():\n",
    "    \n",
    "    if i <= 20:\n",
    "        print(cluster, \" \", cardinality)\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# overall cardinality of most problematic cluster\n",
    "overall_prob_cluster = []\n",
    "\n",
    "for read, cluster in final_dataset.items():\n",
    "    if cluster == max_cluster:\n",
    "        overall_prob_cluster.append(read)\n",
    "\n",
    "print(len(overall_prob_cluster))\n",
    "print(len(overall_prob_cluster) - max_cardinality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything TORNA --> Now I verify if indeed all problematic reads are contained in clusters that ONLY CONTAIN PROBLEMATIC READS, that why they are problematic!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEVI SCORRERE TUTTO IL DIZIONARIO FINAL DATASET, SE IL CLUSTER E' PROBLEMATICO ALLORA VADO AD INSERIRE LA READ IN UNA LISTA DI READ DI QUEL CLUSTER\n",
    "## FINITO DI COSTRUIRE LE LISTE CON LE READS CONTENUTE NEI CLUSTER PROBLEMATICI (UNA LISTA PER CLUSTER PROBLEMATICO) VERIFICO CHE IL NUMERO DI READ E' EFFETTIVAMENTE PARI ALLA CARDINALITA' DEL NUMERO DI READ PROBLEMATICO\n",
    "## QUINDI POSSIAMO CONCLUDERE CHE I CLUSTER PROBLEMATICI SONO QUELLI IN CUI EFFETTIVAMENTE TUTTE LE READ HANNO CLASSE 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "\n",
    "for read, cluster in final_dataset.items():\n",
    "    if cluster in cluster_dict.keys():\n",
    "        cluster_dict[cluster].append(read)\n",
    "    \n",
    "    else:\n",
    "        cluster_dict[cluster] = []\n",
    "        cluster_dict[cluster].append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:  15  total reads:  1  patological reads:  1\n",
      "cluster:  18  total reads:  2  patological reads:  2\n",
      "cluster:  38  total reads:  4  patological reads:  4\n",
      "cluster:  63  total reads:  1  patological reads:  1\n",
      "cluster:  68  total reads:  3  patological reads:  3\n",
      "cluster:  74  total reads:  5  patological reads:  5\n",
      "total patological clusters:  28622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pat_count = 0\n",
    "i = 0\n",
    "\n",
    "for cluster in cluster_dict.keys():\n",
    "    \n",
    "    if cluster in patological_clusters.keys():\n",
    "        \n",
    "        if len(cluster_dict[cluster]) == int(patological_clusters_cardinality[cluster]):\n",
    "            \n",
    "            if i <= 100:\n",
    "                print(\"cluster: \", cluster, \" total reads: \", len(cluster_dict[cluster]), \" patological reads: \", patological_clusters_cardinality[cluster] )\n",
    "                pat_count = pat_count + 1\n",
    "                \n",
    "            else:\n",
    "                pat_count = pat_count + 1\n",
    "        \n",
    "        else:\n",
    "            print(\"Something is wrong!\")\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "# for debugging\n",
    "print(\"total patological clusters: \", pat_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load classifier result function\n",
    "It returns a dictionary where:\n",
    "\n",
    "- key = read id\n",
    "- value = class found by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier_result(path):\n",
    "\n",
    "    classification = open(path, 'r')\n",
    "    classifier_results = {}\n",
    "\n",
    "    for line in classification:\n",
    "        col = []\n",
    "        for j in range(0, len(line.split())):\n",
    "            col.append(line.split()[j])\n",
    "        \n",
    "        classifier_results[col[0]] = col[1]\n",
    "\n",
    "    classification.close()\n",
    "\n",
    "    return classifier_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non classified reads function\n",
    "It returns a list with two elements:\n",
    "\n",
    "- dict of with reads_id of non classified reads\n",
    "- number of non classified reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_classified_reads(classifier_output):\n",
    "    \n",
    "    non_classified_reads = {}\n",
    "    \n",
    "    for read in classifier_output.keys():\n",
    "        if classifier_output[read] == '0':\n",
    "            non_classified_reads[read] = ''\n",
    "     \n",
    "    return [non_classified_reads, len(non_classified_reads)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection function\n",
    "It returns a list with the reads present in both dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(dict1, dict2):\n",
    "    \n",
    "    common_reads = []\n",
    "    \n",
    "    for read in dict1.keys():\n",
    "        if read in dict2.keys():\n",
    "            common_reads.append(read)\n",
    "    \n",
    "    return common_reads        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference function\n",
    "It returns a list of reads that are present in dict1 and not in dict2, NOT the reads in dict2 that are not present in dict1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  difference(dict1, dict2):\n",
    "    \n",
    "    non_shared_reads = []\n",
    "    \n",
    "    for read in dict1.keys():\n",
    "        if read not in dict2.keys():\n",
    "            non_shared_reads.append(read)\n",
    "    \n",
    "    return non_shared_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find general dict function\n",
    "Given a list of dicts, it return a dict of (key, value pairs) where:\n",
    "- key is a key found in any of the dicts in input,\n",
    "- value is a list with the indexes of the dicts where the key appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_general_dict(dict_list):\n",
    "    \n",
    "    # dicts contained in dict_list\n",
    "    num_dict = len(dict_list)\n",
    "    \n",
    "    general_dict = {}\n",
    "    \n",
    "    for i in range(0, num_dict): # scan all dicts in input\n",
    "        \n",
    "        for read in dict_list[i].keys(): # scan each read in the examinated dict \n",
    "            \n",
    "            if read not in general_dict.keys(): # if the read doesn't already exist in the general_dict then create a list and append the id of the dict where the read appears\n",
    "                general_dict[read] = []\n",
    "                general_dict[read].append(str(i))\n",
    "            else: # if the read already exists in the general_dict then just append the id of the dict where the read appears\n",
    "                general_dict[read].append(str(i))\n",
    "    \n",
    "    return general_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, datset_format : bool):\n",
    "\n",
    "    dataset = open(path, \"r\")\n",
    "    dataset_lines = []\n",
    "\n",
    "    if (datset_format):\n",
    "        divisor = 2\n",
    "    else:\n",
    "        divisor = 4\n",
    "\n",
    "    index = 0\n",
    "    for line in dataset:\n",
    "        if (index%divisor==0):\n",
    "            read_id = line.split()[0]\n",
    "            dataset_lines.append(read_id[1: len(read_id)-2])\n",
    "        index = index + 1\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    return dataset_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clusters_result(path):\n",
    "\n",
    "    clusters = open(path, \"r\")\n",
    "    clusters_list = []\n",
    "\n",
    "    for group in clusters:\n",
    "        clusters_list.append(int(group))\n",
    "\n",
    "    clusters.close()\n",
    "\n",
    "    return clusters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset, clusters):\n",
    "    \n",
    "    final_dataset = {}\n",
    "    \n",
    "    for i in range(0, len(dataset)):\n",
    "        final_dataset[dataset[i]] = clusters[i]\n",
    "        \n",
    "    return final_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
