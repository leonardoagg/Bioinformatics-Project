{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data paths:\n",
    "\n",
    "# Gioia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../Bio_Project/SimDataset/reads_datasets/all_250000_1.fq\"\n",
    "#classifier_path = \"../../Bio_Project/SimDataset/classifiers_results/strex_centrifuge_250000.res\"\n",
    "#classifier_path = \"../../Bio_Project/SimDataset/classifiers_results/strex_kraken2_250000.res\"\n",
    "classifier_path = \"../../Bio_Project/SimDataset/classifiers_results/strex_clark_genus_250000.res\"\n",
    "clusters_path =  \"../../Bio_Project/results/LiME_binning_all_250000_1/all_250000_1+RC.fasta.a16.t20.txt\"\n",
    "\n",
    "IsFasta = False\n",
    "\n",
    "TotalReassignment = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"../all_118309_1.fq\"\n",
    "#clusters_path =  \"../all_118309_1+RC.fasta.a16.t20.txt\"\n",
    "#classifier_path = \"../strex_kraken1_118309.res\"\n",
    "\n",
    "dataset_path = \"../all_250000_1.fq\"\n",
    "clusters_path =  \"../all_250000_1+RC.fasta.a16.t20.txt\"\n",
    "#classifier_path = \"../strex_kraken2_250000.res\"\n",
    "classifier_path = \"../strex_centrifuge_250000.res\"\n",
    "\n",
    "IsFasta = False\n",
    "\n",
    "TotalReassignment = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset function \n",
    "- If dataset is in fasta format then dataset_format parameter equals TRUE.\n",
    "- If dataset is in fastq format then dataset_format equals FALSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, datset_format : bool):\n",
    "\n",
    "    dataset = open(path, \"r\")\n",
    "    dataset_lines = []\n",
    "\n",
    "    if (datset_format):\n",
    "        divisor = 2\n",
    "    else:\n",
    "        divisor = 4\n",
    "\n",
    "    index = 0\n",
    "    for line in dataset:\n",
    "        if (index%divisor==0):\n",
    "            read_id = line.split()[0]\n",
    "            dataset_lines.append(read_id[1: len(read_id)-2])\n",
    "        index = index + 1\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    return dataset_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load clusters result function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clusters_result(path):\n",
    "\n",
    "    clusters = open(path, \"r\")\n",
    "    clusters_list = []\n",
    "\n",
    "    for group in clusters:\n",
    "        clusters_list.append(int(group))\n",
    "\n",
    "    clusters.close()\n",
    "\n",
    "    return clusters_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load classifier result function\n",
    "It returns a dictionary where:\n",
    "\n",
    "- key = read id\n",
    "- value = class found by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier_result(path):\n",
    "\n",
    "    classification = open(path, 'r')\n",
    "    classifier_results = {}\n",
    "\n",
    "    for line in classification:\n",
    "        col = []\n",
    "        for j in range(0, len(line.split())):\n",
    "            col.append(line.split()[j])\n",
    "        \n",
    "        classifier_results[col[0]] = col[1]\n",
    "\n",
    "    classification.close()\n",
    "\n",
    "    return classifier_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dataset function\n",
    "This functions creates a list of following lists:\n",
    "[read_id, class, cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset_ids,clusters,classifier):\n",
    "    final_dataset =[]\n",
    "        \n",
    "    for i in range(0, len(dataset_ids)):\n",
    "        final_dataset.append([])\n",
    "\n",
    "        if dataset_ids[i] in classifier:\n",
    "            final_dataset[i].append(dataset_ids[i])\n",
    "            final_dataset[i].append(classifier[dataset_ids[i]])\n",
    "            final_dataset[i].append(clusters[i])\n",
    "        else:\n",
    "            final_dataset[i].append(dataset_ids[i])\n",
    "            final_dataset[i].append(0)\n",
    "            final_dataset[i].append(clusters[i])\n",
    "        \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get inverted index function\n",
    "This function is used to create a list for each cluster. This list represents corresponding classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(clusters, dataset):\n",
    "\n",
    "    num_clusters = max(clusters) + 1\n",
    "    num_reads = len(dataset)\n",
    "    inverted_index = []\n",
    "    \n",
    "    for i in range(0, num_clusters):\n",
    "        inverted_index.append([])\n",
    "   \n",
    "    for i in range(0, num_reads):\n",
    "        inverted_index[dataset[i][2]].append(dataset[i][1])\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find labels function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(read_ids, classification_output):\n",
    "\n",
    "    complete_classifier_result = []\n",
    "    found : bool = False\n",
    "    count = 0\n",
    "    for i in range(0, len(read_ids)):\n",
    "        for j in range(0, len(classification_output)):\n",
    "            col = []\n",
    "            if (read_ids[i] == classification_output[j][0]):\n",
    "                complete_classifier_result.append(classification_output[j])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            count = count +1\n",
    "            complete_classifier_result.append(read_ids[i], '0')\n",
    "\n",
    "    print(\"count: \", count)\n",
    "    print(\"length classification output: \", len(classification_output))\n",
    "    print(\"length complete classification output: \", len(complete_classifier_result))\n",
    "\n",
    "    return complete_classifier_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find labels frequency function\n",
    "Given a cluster, it returns a dictionary which contains the number of occurency for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict --> (class, occurences)\n",
    "\n",
    "def frequency_search(cluster):\n",
    "    \n",
    "    label_dict= {}\n",
    "       \n",
    "    for label in cluster:         \n",
    "    \n",
    "        if label in list(label_dict):\n",
    "            label_dict[label] = label_dict[label] + 1\n",
    "        else:\n",
    "            # if it does not exist, it's automatically created\n",
    "            label_dict[label] = 1\n",
    "        \n",
    "    return label_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic operation functions\n",
    "- intersection\n",
    "- binary search\n",
    "- diff\n",
    "- binary search list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "# Iterative Binary Search Function\n",
    "# It returns index of x in given array arr if present,\n",
    "# else returns -1\n",
    "def binary_search(arr, x):\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        # If x is greater, ignore left half\n",
    "        if arr[mid] < x:\n",
    "            low = mid + 1\n",
    "        # If x is smaller, ignore right half\n",
    "        elif arr[mid] > x:\n",
    "            high = mid - 1\n",
    "        # means x is present at mid\n",
    "        else:\n",
    "            return mid\n",
    "    # If we reach here, then the element was not present\n",
    "    return -1\n",
    "\n",
    "\n",
    "# Python code t get difference of two lists\n",
    "# Not using set()\n",
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif\n",
    "\n",
    "# Iterative Binary Search Function\n",
    "# It returns index of x in given array arr if present,\n",
    "# else returns -1\n",
    "def binary_search_list(arr, x):\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        #print(arr[mid][0])\n",
    "        # If x is greater, ignore left half\n",
    "        if arr[mid][0] < x:\n",
    "            low = mid + 1\n",
    "        # If x is smaller, ignore right half\n",
    "        elif arr[mid][0] > x:\n",
    "            high = mid - 1\n",
    "        # means x is present at mid\n",
    "        else:\n",
    "            return mid\n",
    "    # If we reach here, then the element was not present\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassignment functions\n",
    "1)The first function substitutes every cluster's element (class) with the most the most numerous class\n",
    "2)The second one substitutes only missclassified elements with the most numerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_reassignment(dataset, max_labels):\n",
    "\n",
    "    reassigned_classification = []\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        cluster_index = dataset[i][2]\n",
    "        reassigned_classification.append([dataset[i][0], max_labels[cluster_index]])\n",
    "                                          \n",
    "    return reassigned_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_reassignment(dataset, max_labels):\n",
    "\n",
    "    reassigned_classification = []\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        cluster_index = dataset[i][2]\n",
    "        if (dataset[i][1] == '0' or  dataset[i][1] == 0):\n",
    "            reassigned_classification.append([dataset[i][0], max_labels[cluster_index]])\n",
    "        else:\n",
    "            reassigned_classification.append([dataset[i][0], dataset[i][1]])\n",
    "                                          \n",
    "    return reassigned_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded and datasets created in time:  13.410622835159302\n",
      "Inverted index created in time:  0.8894500732421875\n",
      "Classes have been elaborated in:  4.991955757141113\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "dataset_lines = load_dataset(dataset_path, IsFasta)\n",
    "clusters_list = load_clusters_result(clusters_path)\n",
    "classifier_results = load_classifier_result(classifier_path)\n",
    "\n",
    "#new Structure : id , classifier result , cluster\n",
    "\n",
    "dataset = build_dataset(dataset_lines,clusters_list,classifier_results)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Files loaded and datasets created in time: \", stop-start)\n",
    "\n",
    "\n",
    "\n",
    "if (len(dataset_lines) == len(clusters_list)):\n",
    "    num_reads = len(dataset_lines)\n",
    "else:\n",
    "    print (\"Error in input files!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "    \n",
    "# INVERTED INDEX\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "inverted_index = get_inverted_index(clusters_list, dataset)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Inverted index created in time: \", stop-start)\n",
    "    \n",
    "    \n",
    "start = time.time()\n",
    "i = 0\n",
    "max_label_per_cluster_list = []\n",
    "max_label_list = []\n",
    "for cluster in inverted_index:\n",
    "    label_dict = frequency_search(cluster)\n",
    "    max_label = \"\"\n",
    "    max_count = 0\n",
    "    for label, count in label_dict.items():\n",
    "        #print(label, count)\n",
    "        if (count > max_count):\n",
    "            max_count = count\n",
    "            max_label = label\n",
    "    #print(\"CLUSTER: \", i, \" MAX_LABEL: \", max_label, \"MAX COUNT: \", max_count, \"CLUSTER LENGTH: \", len(cluster))\n",
    "    max_label_list.append(max_label)\n",
    "    max_label_per_cluster_list.append([max_label, max_count, len(cluster)])\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "#Choose the type of reassignment\n",
    "if TotalReassignment:\n",
    "    reassigned_classification = total_reassignment(dataset, max_label_list) \n",
    "else:\n",
    "    reassigned_classification = partial_reassignment(dataset, max_label_list) \n",
    "stop = time.time()\n",
    "\n",
    "\n",
    "print(\"Classes have been elaborated in: \", stop-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  all_250000_1.fq.totalReassignment.res  created.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../../Bio_Project/SimDataset/reads_datasets/all_250000_1.fq\"\n",
    "\n",
    "starting_point = 0\n",
    "\n",
    "for i in range(len(dataset_path)):\n",
    "    if dataset_path[i] == '/':\n",
    "        starting_point = i+1\n",
    "\n",
    "outputfile = dataset_path[starting_point:len(dataset_path)]\n",
    "    \n",
    "    \n",
    "\n",
    "if TotalReassignment:\n",
    "    outputfile = outputfile + \".totalReassignment.res\"\n",
    "else:\n",
    "    outputfile = outputfile + \".partialReassignment.res\"\n",
    "\n",
    "\n",
    "f = open(outputfile, \"w\")\n",
    "\n",
    "for element in reassigned_classification:\n",
    "    f.write(element[0])\n",
    "    f.write(\"\\t\")\n",
    "    f.write(str(element[1]))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "print(\"File \",outputfile ,\" created.\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
