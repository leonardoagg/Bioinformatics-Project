{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data paths:\n",
    "\n",
    "# Gioia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../Bio_Project/SimDataset/reads_datasets/all_250000_1.fq\"\n",
    "#classifier_path = \"../../Bio_Project/SimDataset/classifiers_results/strex_centrifuge_250000.res\"\n",
    "#classifier_path = \"../../Bio_Project/SimDataset/classifiers_results/strex_kraken2_250000.res\"\n",
    "classifier_path = \"../../Bio_Project/SimDataset/classifiers_results/strex_clark_genus_250000.res\"\n",
    "clusters_path =  \"../../Bio_Project/results/LiME_binning_all_250000_1/all_250000_1+RC.fasta.a16.t20.txt\"\n",
    "\n",
    "IsFasta = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"../all_118309_1.fq\"\n",
    "#clusters_path =  \"../all_118309_1+RC.fasta.a16.t20.txt\"\n",
    "#classifier_path = \"../strex_kraken1_118309.res\"\n",
    "\n",
    "dataset_path = \"../all_250000_1.fq\"\n",
    "clusters_path =  \"../all_250000_1+RC.fasta.a16.t20.txt\"\n",
    "#classifier_path = \"../strex_kraken2_250000.res\"\n",
    "classifier_path = \"../strex_centrifuge_250000.res\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "IsFasta = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset function \n",
    "- If dataset is in fasta format then dataset_format parameter equals TRUE.\n",
    "- If dataset is in fastq format then dataset_format equals FALSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, datset_format : bool):\n",
    "\n",
    "    dataset = open(path, \"r\")\n",
    "    dataset_lines = []\n",
    "\n",
    "    if (datset_format):\n",
    "        divisor = 2\n",
    "    else:\n",
    "        divisor = 4\n",
    "\n",
    "    index = 0\n",
    "    for line in dataset:\n",
    "        if (index%divisor==0):\n",
    "            read_id = line.split()[0]\n",
    "            dataset_lines.append(read_id[1: len(read_id)-2])\n",
    "        index = index + 1\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    return dataset_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load clusters result function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clusters_result(path):\n",
    "\n",
    "    clusters = open(path, \"r\")\n",
    "    clusters_list = []\n",
    "\n",
    "    for group in clusters:\n",
    "        clusters_list.append(int(group))\n",
    "\n",
    "    clusters.close()\n",
    "\n",
    "    return clusters_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load classifier result function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier_result(path):\n",
    "\n",
    "    classification = open(path, 'r')\n",
    "    classifier_results = {}\n",
    "\n",
    "    for line in classification:\n",
    "        col = []\n",
    "        for j in range(0, len(line.split())):\n",
    "            col.append(line.split()[j])\n",
    "        #read_id = line.split()[0]\n",
    "        #class_id = line.split()[1]\n",
    "        #col.append(read_id)\n",
    "        #col.append(class_id)\n",
    "        \n",
    "        classifier_results[col[0]] = col[1]\n",
    "\n",
    "    classification.close()\n",
    "\n",
    "    return classifier_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get inverted index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(clusters, read_ids):\n",
    "\n",
    "    num_clusters = max(clusters) + 1\n",
    "    num_reads = len(read_ids)\n",
    "    inverted_index = []\n",
    "    \n",
    "    for i in range(0, num_clusters):\n",
    "        inverted_index.append([])\n",
    "   \n",
    "    #try to append in order\n",
    "    for i in range(0, num_reads):\n",
    "        inverted_index[read_ids[i][2]].append(read_ids[i][1])\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find labels function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(read_ids, classification_output):\n",
    "\n",
    "    complete_classifier_result = []\n",
    "    found : bool = False\n",
    "    count = 0\n",
    "    for i in range(0, len(read_ids)):\n",
    "        for j in range(0, len(classification_output)):\n",
    "            col = []\n",
    "            if (read_ids[i] == classification_output[j][0]):\n",
    "                complete_classifier_result.append(classification_output[j])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            count = count +1\n",
    "            complete_classifier_result.append(read_ids[i], '0')\n",
    "\n",
    "    print(\"count: \", count)\n",
    "    print(\"length classification output: \", len(classification_output))\n",
    "    print(\"length complete classification output: \", len(complete_classifier_result))\n",
    "\n",
    "    return complete_classifier_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find labels frequency function\n",
    "This function takes in input a cluster (group of reads, each read is identified by its read_id) and the classification of reads performed by a classifier\n",
    "and return a dictionary composed by key-value pairs where the key is a label and value equals to the frequency of such label in the cluster provided in input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ricorda gestione non presente --> classe 0\n",
    "# dict --> (classe, volte che la vedo nel cluster)\n",
    "\n",
    "def frequency_search_binary(cluster):\n",
    "    \n",
    "    label_dict= {}\n",
    "       \n",
    "    for read in cluster:         \n",
    "    \n",
    "        if read in list(label_dict):\n",
    "            label_dict[read] = label_dict[read] + 1\n",
    "        else:\n",
    "            # if it does not exist, it's automatically created\n",
    "            label_dict[read] = 1\n",
    "        \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find lonely reads function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lonely_reads(clusters):\n",
    "\n",
    "    lonely_reads = []\n",
    "\n",
    "    for i in range(0, len(clusters)):\n",
    "        if (len(clusters[i]) == 1):\n",
    "            lonely_reads.append(clusters[i][0])\n",
    "\n",
    "    return lonely_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find non classified reads function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_classified_reads(read_ids, classification_output):\n",
    "\n",
    "    non_classified_reads = []\n",
    "\n",
    "    if (len(read_ids) > len(classification_output)):\n",
    "        print(\"le read non classificate non sono presenti nel file di output, DA IMPLEMENTARE\")\n",
    "    elif (len(read_ids) == len(classification_output)):\n",
    "        ###print(\"le read non classificate hanno label = 0\")\n",
    "        for i in range(0, len(classification_output)):\n",
    "            if (classification_output[i][1] == '0'):\n",
    "                non_classified_reads.append(classification_output[i][0])\n",
    "    else:\n",
    "        print(\"Error in input files!\")\n",
    "\n",
    "    return non_classified_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic operation functions\n",
    "- intersection\n",
    "- binary search\n",
    "- insertion sort\n",
    "- diff\n",
    "- binary search list\n",
    "- insertion sort list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "# Iterative Binary Search Function\n",
    "# It returns index of x in given array arr if present,\n",
    "# else returns -1\n",
    "def binary_search(arr, x):\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        # If x is greater, ignore left half\n",
    "        if arr[mid] < x:\n",
    "            low = mid + 1\n",
    "        # If x is smaller, ignore right half\n",
    "        elif arr[mid] > x:\n",
    "            high = mid - 1\n",
    "        # means x is present at mid\n",
    "        else:\n",
    "            return mid\n",
    "    # If we reach here, then the element was not present\n",
    "    return -1\n",
    "\n",
    "\n",
    "def insertion_sort(arr):\n",
    "        \n",
    "    for i in range(len(arr)):\n",
    "        cursor = arr[i]\n",
    "        pos = i  \n",
    "        while pos > 0 and arr[pos - 1] > cursor:\n",
    "            # Swap the number down the list\n",
    "            arr[pos] = arr[pos - 1]\n",
    "            pos = pos - 1\n",
    "        # Break and do the final swap\n",
    "        arr[pos] = cursor\n",
    "    return arr\n",
    "\n",
    "# Python code t get difference of two lists\n",
    "# Not using set()\n",
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif\n",
    "\n",
    "# Iterative Binary Search Function\n",
    "# It returns index of x in given array arr if present,\n",
    "# else returns -1\n",
    "def binary_search_list(arr, x):\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        #print(arr[mid][0])\n",
    "        # If x is greater, ignore left half\n",
    "        if arr[mid][0] < x:\n",
    "            low = mid + 1\n",
    "        # If x is smaller, ignore right half\n",
    "        elif arr[mid][0] > x:\n",
    "            high = mid - 1\n",
    "        # means x is present at mid\n",
    "        else:\n",
    "            return mid\n",
    "    # If we reach here, then the element was not present\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "def insertion_sort_list(arr):\n",
    "        \n",
    "    for i in range(len(arr)):\n",
    "        cursor = arr[i][0]\n",
    "        pos = i\n",
    "        while pos > 0 and arr[pos - 1][0] > cursor:\n",
    "            # Swap the number down the list\n",
    "            arr[pos][0] = arr[pos - 1][0]\n",
    "            pos = pos - 1\n",
    "        # Break and do the final swap\n",
    "        arr[pos][0] = cursor\n",
    "\n",
    "    return arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find labels (improved) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels_improved(read_ids, classification_output):\n",
    "\n",
    "    complete_classifier_result = []\n",
    "    found : bool = False\n",
    "    count = 0\n",
    "\n",
    "    classification_output = insertion_sort(classification_output)\n",
    "\n",
    "    print(\"classification_output sorted\")\n",
    "\n",
    "    for i in range(0, len(read_ids)):\n",
    "        position = binary_search_list(classification_output, str(read_ids[i]))\n",
    "        if (position == -1):\n",
    "            count = count + 1\n",
    "            complete_classifier_result.append([read_ids[i], '0'])\n",
    "        else:\n",
    "            complete_classifier_result.append(classification_output[position])\n",
    "\n",
    "    print(\"count: \", count)\n",
    "    print(\"length classification output: \", len(classification_output))\n",
    "    print(\"length complete classification output: \", len(complete_classifier_result))\n",
    "\n",
    "    return complete_classifier_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassignment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassignment(dataset, max_labels):\n",
    "\n",
    "    reassigned_classification = []\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        cluster_index = dataset[i][2]\n",
    "        reassigned_classification.append([dataset[i][0], max_labels[cluster_index]])\n",
    "                                          \n",
    "    return reassigned_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset_ids,clusters,classifier):\n",
    "    final_dataset =[]\n",
    "        \n",
    "    for i in range(0, len(dataset_ids)):\n",
    "        final_dataset.append([])\n",
    "\n",
    "        if dataset_ids[i] in classifier:\n",
    "            final_dataset[i].append(dataset_ids[i])\n",
    "            final_dataset[i].append(classifier[dataset_ids[i]])\n",
    "            final_dataset[i].append(clusters[i])\n",
    "        else:\n",
    "            final_dataset[i].append(dataset_ids[i])\n",
    "            final_dataset[i].append(0)\n",
    "            final_dataset[i].append(clusters[i])\n",
    "        \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for loading and creating datasets:  25.532742023468018\n",
      "Time for getting inverted index:  1.7932662963867188\n",
      "Time for class definition:  10.827752590179443\n",
      "Time for class definition:  0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "dataset_lines = load_dataset(dataset_path, IsFasta)\n",
    "clusters_list = load_clusters_result(clusters_path)\n",
    "classifier_results = load_classifier_result(classifier_path)\n",
    "\n",
    "#new Structure : id , classifier result , cluster\n",
    "\n",
    "dataset = build_dataset(dataset_lines,clusters_list,classifier_results)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Time for loading and creating datasets: \", stop-start)\n",
    "\n",
    "\n",
    "\n",
    "if (len(dataset_lines) == len(clusters_list)):\n",
    "    num_reads = len(dataset_lines)\n",
    "else:\n",
    "    print (\"Error in input files!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# INVERTED INDEX\n",
    "inverted_index = get_inverted_index(clusters_list, dataset)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Time for getting inverted index: \", stop-start)\n",
    "    \n",
    "    \n",
    "start = time.time()\n",
    "i = 0\n",
    "max_label_per_cluster_list = []\n",
    "max_label_list = []\n",
    "for cluster in inverted_index:\n",
    "    label_dict = frequency_search_binary(cluster)\n",
    "    max_label = \"\"\n",
    "    max_count = 0\n",
    "    for label, count in label_dict.items():\n",
    "        #print(label, count)\n",
    "        if (count > max_count):\n",
    "            max_count = count\n",
    "            max_label = label\n",
    "    #print(\"CLUSTER: \", i, \" MAX_LABEL: \", max_label, \"MAX COUNT: \", max_count, \"CLUSTER LENGTH: \", len(cluster))\n",
    "    max_label_list.append(max_label)\n",
    "    max_label_per_cluster_list.append([max_label, max_count, len(cluster)])\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "reassigned_classification = reassignment(dataset, max_label_list) \n",
    "\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Time for class definition: \", stop-start)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Time for class definition: \", stop-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ClassificationReassignedNotOrdered.res\", \"w\")\n",
    "\n",
    "for element in reassigned_classification:\n",
    "    f.write(element[0])\n",
    "    f.write(\"\\t\")\n",
    "    f.write(str(element[1]))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
